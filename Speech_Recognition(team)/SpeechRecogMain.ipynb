{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecogMain",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5FQurIy033S",
        "colab_type": "code",
        "outputId": "4950f311-0bc2-42b1-87a1-7892bdb5658b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "!pip install tensorflow==1.13.2\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 65kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.29.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (47.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ6EHZbE5krp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHTllkdsPCaE",
        "colab_type": "code",
        "outputId": "55b5e44d-56c1-451f-d1e3-60811ed28771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "!pip install utils\n",
        "import utils\n",
        "!pip install core\n",
        "import core as flags_core\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting utils\n",
            "  Downloading https://files.pythonhosted.org/packages/55/e6/c2d2b2703e7debc8b501caae0e6f7ead148fd0faa3c8131292a599930029/utils-1.0.1-py2.py3-none-any.whl\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement core (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for core\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5353f1ed4fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install core'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflags_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiuUkORN36JB",
        "colab_type": "code",
        "outputId": "2b6409dd-aae4-4c85-e233-978d5bc13618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/models\")\n",
        "!export PYTHONPATH=$PYTHONPATH:/content/models\n",
        "import tensorflow as tf\n",
        "from absl import app as absl_app\n",
        "from absl import flags\n",
        "#import data.dataset as dataset\n",
        "#import decoder\n",
        "#import speechrecogModel\n",
        "from flags import core as flags_core\n",
        "from official.utils.logs import hooks_helper\n",
        "from official.utils.logs import logger\n",
        "from official.utils.misc import distribution_utils\n",
        "from official.utils.misc import model_helpers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d31e6abaac9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#import decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#import speechrecogModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mflags_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhooks_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flags'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zln48Pde4ZZI",
        "colab_type": "code",
        "outputId": "e372be3c-0196-4e05-92e8-93c392ed72e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "VOCAB_FILE = os.path.join(os.path.dirname(__file__), \"data/vocabulary.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c247f3099181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVOCAB_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/vocabulary.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U39PHuO0k0pR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ctc_loss(label_len, ctc_X_length, labels, logits):\n",
        "  label_len = tf.to_int32(tf.squeeze(label_len))\n",
        "  ctc_X_length = tf.to_int32(tf.squeeze(ctc_X_length))\n",
        "  sparse_labels = tf.to_int32(tf.keras.backend.ctc_label_dense_to_sparse(labels, label_len))\n",
        "  y_pred = tf.log(tf.transpose(logits, perm=[1, 0, 2]) + tf.keras.backend.epsilon())\n",
        "  return tf.expand_dims(tf.nn.ctc_loss(labels=sparse_labels, inputs=y_pred,sequence_length=ctc_X_length),axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE9XREFnpBwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(estimator,speech_labels,entries,X_fn_eval):\n",
        "  predictions=estimator.predict(input_fn=X_fn_eval)\n",
        "  probabilities=[pred[\"probabilities\"] for pred in predictions]\n",
        "  num_examples=len(probs)\n",
        "  targets=[entry[2] for entry in entries]\n",
        "  total_wer=0\n",
        "  greedy_decoder=decoder.DeepSpeechDecoder(speech_labels)\n",
        "  for i in range(num_examples):\n",
        "    decoded_string=greedy_decoder.decode(probs[i])\n",
        "    total_wer+=greedy_decoder.wer(decoded_string,targets[i])/float(len(targets[i].split()))\n",
        "  total_wer/=num_examples\n",
        "  global_step = estimator.get_variable_value(tf.GraphKeys.GLOBAL_STEP)\n",
        "  results={\"WER\":total_wer,tf.GraphKeys.GLOBAL_STEP:Global_step}\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKDQSEae4ebM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length_after_conv(max_time_steps,ctc_time_steps,X_length):\n",
        "  ctc_X_length = tf.to_float(tf.multiply(X_length, ctc_time_steps))\n",
        "  return tf.to_int32(tf.floordiv(ctc_X_length, tf.to_float(max_time_steps)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p0pmbE3-F-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model(features,labels,mode,params):\n",
        "  num_classes=params[\"num_classes\"]\n",
        "  X_length=features[\"X_length\"]\n",
        "  label_length=features[\"label_length\"]\n",
        "  features=features[\"features\"]\n",
        "  model=speechrecogModel.DeepSpeechModel(flag_obj.rnn_layers_count,flags_obj.rnn_type,flags_obj.is_Bidirectional,flags_obj.rnn_hidden_size,num_classes,flags_obj.bias)\n",
        "  if(mode==tf.estimator.ModeKeys.PREDICT):\n",
        "    logits=model(features,training=False)\n",
        "    predictions={\n",
        "        \"classes\":tf.argmax(logits,axis=2),\n",
        "        \"probabilities\":tf.nn.softmax(logits),\n",
        "        \"logits\":logits\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode,predications)\n",
        "  logits=model(features,training=True)\n",
        "  probabilities=tf.nn.softmax(logits)\n",
        "  ctc_X_length=length_after_conv(tf.shape(features)[1],tf.shape(probabilities)[1],X_length)\n",
        "  loss=tf.reduce_mean(ctc_loss(label_length,ctc_X_length,labels,probabilities))\n",
        "  optimizer=tf.train.AdamOptimizer(learning_rate=flags_obj.learning_rate)\n",
        "  Global_step=tf.train.get_or_create_global_step()\n",
        "  minimize=optimizer.minimize(loss,Global_step)\n",
        "  update=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "  train=tf.group(minimize,update)\n",
        "  return tf.estimator.EstimatorSpec(mode,loss,train)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm9CVbamORDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(data_dir):\n",
        "  audio_conf = dataset.AudioConfig(sample_rate=flags_obj.sample_rate,window_ms=flags_obj.window_ms,stride_ms=flags_obj.stride_ms,normalize=True)\n",
        "  training_data_conf = dataset.DatasetConfig(audio_conf,data_dir,flags_obj.vocabulary_file,flags_obj.sortagrad)\n",
        "  speech_data = dataset.DeepSpeechDataset(training_data_conf)\n",
        "  return speech_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgx-7yS1Qsxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_deep_speech(_):\n",
        "  tf.set_random_seed(flags_obj.seed)\n",
        "  tf.logging.info(\"Data preprocessing...\")\n",
        "  train_speech_dataset = generate_dataset(flags_obj.train_data_dir)\n",
        "  eval_speech_dataset = generate_dataset(flags_obj.eval_data_dir)\n",
        "  num_classes = len(train_speech_dataset.speech_labels)\n",
        "\n",
        "  # Use distribution strategy for multi-gpu training\n",
        "  num_gpus = flags_core.get_num_gpus(flags_obj)\n",
        "  distribution_strategy = distribution_utils.get_distribution_strategy(num_gpus=num_gpus)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      train_distribute=distribution_strategy)\n",
        "\n",
        "  estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      model_dir=flags_obj.model_dir,\n",
        "      config=run_config,\n",
        "      params={\n",
        "          \"num_classes\": num_classes,\n",
        "      }\n",
        "  )\n",
        "\n",
        "  run_params = {\n",
        "      \"batch_size\": flags_obj.batch_size,\n",
        "      \"train_epochs\": flags_obj.train_epochs,\n",
        "      \"rnn_hidden_size\": flags_obj.rnn_hidden_size,\n",
        "      \"rnn_layers_count\": flags_obj.rnn_layers_count,\n",
        "      \"rnn_type\": flags_obj.rnn_type,\n",
        "      \"is_Bidirectional\": flags_obj.is_Bidirectional,\n",
        "      \"bias\": flags_obj.bias\n",
        "  }\n",
        "\n",
        "  dataset_name = \"LibriSpeech\"\n",
        "  benchmark_logger = logger.get_benchmark_logger()\n",
        "  benchmark_logger.log_run_info(\"deep_speech\", dataset_name, run_params,\n",
        "                                test_id=flags_obj.benchmark_test_id)\n",
        "\n",
        "  train_hooks = hooks_helper.get_train_hooks(\n",
        "      flags_obj.hooks,\n",
        "      model_dir=flags_obj.model_dir,\n",
        "      batch_size=flags_obj.batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amifiV4Y-c41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def input_train():\n",
        "    return dataset.input_fn(\n",
        "        per_replica_batch_size, train_speech_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbJrmPnP-wOD",
        "colab_type": "code",
        "outputId": "dec64a20-fbb5-45ce-cdfe-1f9afd99e9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "  def input_eval():\n",
        "    return dataset.input_fn(\n",
        "        per_replica_batch_size, eval_speech_dataset)\n",
        "\n",
        "  total_training_cycle = (flags_obj.train_epochs // flags_obj.epochs_between_evals)\n",
        "  for cycle_index in range(total_training_cycle):\n",
        "    tf.logging.info(\"Starting a training cycle: %d/%d\",\n",
        "                    cycle_index + 1, total_training_cycle)\n",
        "\n",
        "    train_speech_dataset.entries = dataset.batch_wise_dataset_shuffle(\n",
        "        train_speech_dataset.entries, cycle_index, flags_obj.sortagrad,\n",
        "        flags_obj.batch_size)\n",
        "\n",
        "    estimator.train(input_fn=input_fn_train, hooks=train_hooks)\n",
        "\n",
        "    tf.logging.info(\"Starting to evaluate...\")\n",
        "\n",
        "    eval_results = evaluate_model(\n",
        "        estimator, eval_speech_dataset.speech_labels,\n",
        "        eval_speech_dataset.entries, input_fn_eval)\n",
        "\n",
        "    benchmark_logger.log_evaluation_result(eval_results)\n",
        "    tf.logging.info(\n",
        "        \"Iteration {}: WER = {:.2f}\".format(\n",
        "            cycle_index + 1, eval_results[\"WER\"]))\n",
        "\n",
        "    if model_helpers.past_stop_threshold(\n",
        "        flags_obj.wer_threshold, eval_results[\"WER\"]):\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f4e11dc2764f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m       per_replica_batch_size, eval_speech_dataset)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtotal_training_cycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mflags_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_between_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcycle_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_training_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   tf.logging.info(\"Starting a training cycle: %d/%d\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flags_obj' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klL9NGW--iW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_deep_speech_flags():\n",
        " \n",
        "  flags_core.define_base(\n",
        "      data_dir=False,  \n",
        "      export_dir=True,\n",
        "      train_epochs=True,\n",
        "      hooks=True,\n",
        "      num_gpu=True,\n",
        "      epochs_between_evals=True\n",
        "  )\n",
        "  flags_core.define_performance(\n",
        "      num_parallel_calls=False,\n",
        "      inter_op=False,\n",
        "      intra_op=False,\n",
        "      synthetic_data=False,\n",
        "      max_train_steps=False,\n",
        "      dtype=False\n",
        "  )\n",
        "  flags_core.define_benchmark()\n",
        "  flags.adopt_module_key_flags(flags_core)\n",
        "\n",
        "  flags_core.set_defaults(\n",
        "      model_dir=\"/tmp/deep_speech_model/\",\n",
        "      export_dir=\"/tmp/deep_speech_saved_model/\",\n",
        "      train_epochs=10,\n",
        "      batch_size=128,\n",
        "      hooks=\"\")\n",
        "\n",
        "  # Deep speech flags\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"seed\", default=1,\n",
        "      help=flags_core.help_wrap(\"The random seed.\"))\n",
        "\n",
        "  flags.DEFINE_string(\n",
        "      name=\"train_data_dir\",\n",
        "      default=\"/tmp/librispeech_data/test-clean/LibriSpeech/test-clean.csv\",\n",
        "      help=flags_core.help_wrap(\"The csv file path of train dataset.\"))\n",
        "\n",
        "  flags.DEFINE_string(\n",
        "      name=\"eval_data_dir\",\n",
        "      default=\"/tmp/librispeech_data/test-clean/LibriSpeech/test-clean.csv\",\n",
        "      help=flags_core.help_wrap(\"The csv file path of evaluation dataset.\"))\n",
        "\n",
        "  flags.DEFINE_bool(\n",
        "      name=\"sortagrad\", default=True,\n",
        "      help=flags_core.help_wrap(\n",
        "          \"If true, sort examples by audio length and perform no \"\n",
        "          \"batch_wise shuffling for the first epoch.\"))\n",
        "\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"sample_rate\", default=16000,\n",
        "      help=flags_core.help_wrap(\"The sample rate for audio.\"))\n",
        "\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"window_ms\", default=20,\n",
        "      help=flags_core.help_wrap(\"The frame length for spectrogram.\"))\n",
        "\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"stride_ms\", default=10,\n",
        "      help=flags_core.help_wrap(\"The frame step.\"))\n",
        "\n",
        "  flags.DEFINE_string(\n",
        "      name=\"vocabulary_file\", default=_VOCABULARY_FILE,\n",
        "      help=flags_core.help_wrap(\"The file path of vocabulary file.\"))\n",
        "\n",
        "  # RNN related flags\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"rnn_hidden_size\", default=800,\n",
        "      help=flags_core.help_wrap(\"The hidden size of RNNs.\"))\n",
        "\n",
        "  flags.DEFINE_integer(\n",
        "      name=\"rnn_hidden_layers\", default=5,\n",
        "      help=flags_core.help_wrap(\"The number of RNN layers.\"))\n",
        "\n",
        "  flags.DEFINE_bool(\n",
        "      name=\"use_bias\", default=True,\n",
        "      help=flags_core.help_wrap(\"Use bias in the last fully-connected layer\"))\n",
        "\n",
        "  flags.DEFINE_bool(\n",
        "      name=\"is_bidirectional\", default=True,\n",
        "      help=flags_core.help_wrap(\"If rnn unit is bidirectional\"))\n",
        "\n",
        "  flags.DEFINE_enum(\n",
        "      name=\"rnn_type\", default=\"gru\",\n",
        "      enum_values=deep_speech_model.SUPPORTED_RNNS.keys(),\n",
        "      case_sensitive=False,\n",
        "      help=flags_core.help_wrap(\"Type of RNN cell.\"))\n",
        "\n",
        "  # Training related flags\n",
        "  flags.DEFINE_float(\n",
        "      name=\"learning_rate\", default=5e-4,\n",
        "      help=flags_core.help_wrap(\"The initial learning rate.\"))\n",
        "\n",
        "  # Evaluation metrics threshold\n",
        "  flags.DEFINE_float(\n",
        "      name=\"wer_threshold\", default=None,\n",
        "      help=flags_core.help_wrap(\n",
        "          \"If passed, training will stop when the evaluation metric WER is \"\n",
        "          \"greater than or equal to wer_threshold. For libri speech dataset \"\n",
        "          \"the desired wer_threshold is 0.23 which is the result achieved by \"\n",
        "          \"MLPerf implementation.\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rP0hSA_FsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(_):\n",
        "  with logger.benchmark_context(flags_obj):\n",
        "    run_deep_speech(flags_obj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qol0i8G_I6O_",
        "colab_type": "code",
        "outputId": "e38ea79a-d744-4e9c-f80a-24eee3b9a09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "  define_deep_speech_flags()\n",
        "  flags_obj = flags.FLAGS\n",
        "  absl_app.run(main)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e8818b948efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdefine_deep_speech_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mflags_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mabsl_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-78d13e143a73>\u001b[0m in \u001b[0;36mdefine_deep_speech_flags\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Add flags for run_deep_speech.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Add common flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   flags_core.define_base(\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# we use train_data_dir and eval_data_dir instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mexport_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'flags_core' is not defined"
          ]
        }
      ]
    }
  ]
}
